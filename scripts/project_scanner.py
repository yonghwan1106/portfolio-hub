import os
import json
import re
import requests
from pathlib import Path
from datetime import datetime
import time
from urllib.parse import urljoin

class ProjectScanner:
    def __init__(self, base_path="C:/MYCLAUDE_PROJECT", github_username="yonghwan1106"):
        self.base_path = Path(base_path)
        self.github_username = github_username
        self.github_base_url = f"https://{github_username}.github.io/"
        self.projects_data = []
        
        # ê³µëª¨ì „ í‚¤ì›Œë“œ íŒ¨í„´ ì •ì˜
        self.contest_patterns = {
            "ì§€ìì²´": ["ì‹œì²­", "êµ¬ì²­", "êµ°ì²­", "ê´‘ì—­ì‹œ", "ë„ì²­"],
            "ë†ì—…": ["ë†ì—…", "ë†ì´Œ", "ë†ì–´ì´Œ", "AgriFood"],
            "í™˜ê²½": ["í™˜ê²½", "ì—ë„ˆì§€", "ì¹œí™˜ê²½", "Eco", "ê·¸ë¦°"],
            "ì²­ë…„": ["ì²­ë…„", "Youth", "ì°½ì—…"],
            "ê·œì œí˜ì‹ ": ["ê·œì œ", "í˜ì‹ ", "Innovation"],
            "AI/ë””ì§€í„¸": ["AI", "ë””ì§€í„¸", "ìŠ¤ë§ˆíŠ¸", "GenAI"],
            "ê´€ê´‘": ["ê´€ê´‘", "Tourism", "ì—¬í–‰"],
            "ì•ˆì „": ["ì•ˆì „", "Safety", "ì¬í•´"]
        }
        
        # ìˆ˜ìƒ í‚¤ì›Œë“œ íŒ¨í„´
        self.award_keywords = ["ìˆ˜ìƒ", "ë‹¹ì„ ", "ì„ ì •", "ìš°ìˆ˜ìƒ", "ìµœìš°ìˆ˜ìƒ", "ëŒ€ìƒ", "ì…ìƒ"]

    def scan_all_projects(self):
        """ì „ì²´ í”„ë¡œì íŠ¸ ìŠ¤ìº” ì‹¤í–‰"""
        print("ğŸ” í”„ë¡œì íŠ¸ ìŠ¤ìº”ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        if not self.base_path.exists():
            print(f"âŒ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.base_path}")
            return None
            
        # í´ë” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        project_folders = [f for f in self.base_path.iterdir() 
                          if f.is_dir() and not f.name.startswith('.')]
        
        print(f"ğŸ“ ì´ {len(project_folders)}ê°œ í”„ë¡œì íŠ¸ í´ë” ë°œê²¬")
        
        for i, folder in enumerate(project_folders, 1):
            print(f"\n[{i}/{len(project_folders)}] ğŸ“‚ {folder.name} ë¶„ì„ ì¤‘...")
            project_info = self.analyze_project(folder)
            if project_info:
                self.projects_data.append(project_info)
            time.sleep(0.5)  # API í˜¸ì¶œ ì œí•œ ë°©ì§€
        
        # í†µê³„ ì¶œë ¥
        self.print_statistics()
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        self.save_to_json()
        
        return self.projects_data

    def analyze_project(self, folder_path):
        """ê°œë³„ í”„ë¡œì íŠ¸ ë¶„ì„"""
        project_name = folder_path.name
        project_info = {
            "id": project_name,
            "name": project_name,
            "folder_path": str(folder_path),
            "scan_date": datetime.now().isoformat(),
            "category": self.categorize_project(project_name),
            "status": "analyzed"
        }
        
        try:
            # 1. ë¬¸ì„œ ë¶„ì„
            docs_info = self.analyze_documents(folder_path)
            project_info.update(docs_info)
            
            # 2. ì›¹ì‚¬ì´íŠ¸ ë¶„ì„
            website_info = self.analyze_website(folder_path, project_name)
            project_info.update(website_info)
            
            # 3. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            project_info["quality_score"] = self.calculate_quality_score(project_info)
            
            # 4. ìˆ˜ìƒ ì—¬ë¶€ í™•ì¸
            project_info["award_status"] = self.check_award_status(project_info)
            
            return project_info
            
        except Exception as e:
            print(f"âš ï¸  {project_name} ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            project_info["status"] = "error"
            project_info["error"] = str(e)
            return project_info

    def analyze_documents(self, folder_path):
        """í”„ë¡œì íŠ¸ ë¬¸ì„œ ë¶„ì„"""
        docs_info = {
            "has_readme": False,
            "has_proposal": False,
            "has_website_folder": False,
            "document_files": [],
            "title": "",
            "description": "",
            "contest_info": {}
        }
        
        # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
        all_files = []
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                all_files.append(Path(root) / file)
        
        # ë¬¸ì„œ ë¶„ì„
        for file_path in all_files:
            file_lower = file_path.name.lower()
            
            # README íŒŒì¼ í™•ì¸
            if file_lower.startswith('readme'):
                docs_info["has_readme"] = True
                content = self.read_file_safe(file_path)
                if content:
                    docs_info.update(self.extract_readme_info(content))
            
            # ì œì•ˆì„œ íŒŒì¼ í™•ì¸
            elif any(keyword in file_lower for keyword in ['ì œì•ˆì„œ', 'proposal', 'ê³„íšì„œ']):
                docs_info["has_proposal"] = True
                docs_info["document_files"].append(file_path.name)
            
            # ê¸°íƒ€ ë¬¸ì„œ íŒŒì¼
            elif file_path.suffix.lower() in ['.md', '.txt', '.hwp', '.pdf', '.docx']:
                docs_info["document_files"].append(file_path.name)
        
        # ì›¹ì‚¬ì´íŠ¸ í´ë” í™•ì¸
        website_folders = ['website', 'web', 'html', 'src', 'public']
        for folder_name in website_folders:
            if (folder_path / folder_name).exists():
                docs_info["has_website_folder"] = True
                break
        
        # HTML íŒŒì¼ ì§ì ‘ í™•ì¸
        html_files = [f for f in all_files if f.suffix.lower() in ['.html', '.htm']]
        if html_files:
            docs_info["has_website_folder"] = True
            docs_info["html_files"] = [f.name for f in html_files]
        
        return docs_info

    def analyze_website(self, folder_path, project_name):
        """ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ë° GitHub Pages í™•ì¸"""
        website_info = {
            "github_url": "",
            "is_live": False,
            "response_code": None,
            "has_mobile_responsive": False,
            "estimated_quality": "unknown"
        }
        
        # GitHub Pages URL ìƒì„±
        github_url = f"{self.github_base_url}{project_name}/"
        website_info["github_url"] = github_url
        
        # ì‚¬ì´íŠ¸ ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        try:
            response = requests.get(github_url, timeout=10)
            website_info["response_code"] = response.status_code
            
            if response.status_code == 200:
                website_info["is_live"] = True
                
                # ê°„ë‹¨í•œ í’ˆì§ˆ ì²´í¬
                content = response.text.lower()
                quality_indicators = {
                    "has_title": bool(re.search(r'<title[^>]*>(?!\s*$)', content)),
                    "has_css": 'css' in content or 'style' in content,
                    "has_js": 'script' in content or 'javascript' in content,
                    "has_responsive": 'viewport' in content or 'responsive' in content,
                    "content_length": len(content)
                }
                
                website_info.update(quality_indicators)
                website_info["estimated_quality"] = self.estimate_website_quality(quality_indicators)
            
        except requests.RequestException as e:
            website_info["connection_error"] = str(e)
        
        return website_info

    def categorize_project(self, project_name):
        """í”„ë¡œì íŠ¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        name_lower = project_name.lower()
        
        for category, keywords in self.contest_patterns.items():
            if any(keyword.lower() in name_lower for keyword in keywords):
                return category
        
        return "ê¸°íƒ€"

    def calculate_quality_score(self, project_info):
        """í”„ë¡œì íŠ¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )"""
        score = 0
        
        # ë¬¸ì„œ ì™„ì„±ë„ (40ì )
        if project_info.get("has_readme"):
            score += 20
        if project_info.get("has_proposal"):
            score += 15
        if project_info.get("document_files"):
            score += 5
        
        # ì›¹ì‚¬ì´íŠ¸ ì¡´ì¬ ë° í’ˆì§ˆ (40ì )
        if project_info.get("is_live"):
            score += 20
            
            quality = project_info.get("estimated_quality", "unknown")
            if quality == "high":
                score += 20
            elif quality == "medium":
                score += 15
            elif quality == "low":
                score += 10
        
        # ì¶”ê°€ ìš”ì†Œ (20ì )
        if project_info.get("has_website_folder"):
            score += 10
        if project_info.get("contest_info"):
            score += 5
        if project_info.get("award_status") == "awarded":
            score += 5
        
        return min(score, 100)

    def estimate_website_quality(self, indicators):
        """ì›¹ì‚¬ì´íŠ¸ í’ˆì§ˆ ì¶”ì •"""
        quality_score = 0
        
        if indicators.get("has_title"): quality_score += 1
        if indicators.get("has_css"): quality_score += 1
        if indicators.get("has_js"): quality_score += 1
        if indicators.get("has_responsive"): quality_score += 1
        if indicators.get("content_length", 0) > 5000: quality_score += 1
        
        if quality_score >= 4:
            return "high"
        elif quality_score >= 2:
            return "medium"
        else:
            return "low"

    def check_award_status(self, project_info):
        """ìˆ˜ìƒ ì—¬ë¶€ í™•ì¸"""
        # ë¬¸ì„œ ë‚´ìš©ì—ì„œ ìˆ˜ìƒ í‚¤ì›Œë“œ ê²€ìƒ‰
        text_to_check = f"{project_info.get('title', '')} {project_info.get('description', '')}"
        
        for keyword in self.award_keywords:
            if keyword in text_to_check:
                return "awarded"
        
        # í´ë”ëª…ì—ì„œ í™•ì¸
        folder_name = project_info.get("name", "").lower()
        for keyword in self.award_keywords:
            if keyword in folder_name:
                return "awarded"
        
        return "unknown"

    def extract_readme_info(self, content):
        """README ë‚´ìš©ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        info = {}
        
        # ì œëª© ì¶”ì¶œ
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if title_match:
            info["title"] = title_match.group(1).strip()
        
        # ì„¤ëª… ì¶”ì¶œ (ì²« ë²ˆì§¸ ë‹¨ë½)
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('```'):
                info["description"] = line[:200] + "..." if len(line) > 200 else line
                break
        
        # ê³µëª¨ì „ ì •ë³´ ì¶”ì¶œ
        contest_patterns = [
            r'ê³µëª¨ì „[:\s]*(.+)',
            r'contest[:\s]*(.+)',
            r'ë§ˆê°ì¼[:\s]*(.+)',
            r'ì‹œìƒê¸ˆ[:\s]*(.+)'
        ]
        
        contest_info = {}
        for pattern in contest_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                contest_info[pattern.split('[')[0]] = match.group(1).strip()
        
        if contest_info:
            info["contest_info"] = contest_info
        
        return info

    def read_file_safe(self, file_path):
        """ì•ˆì „í•œ íŒŒì¼ ì½ê¸°"""
        try:
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin-1']
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read()
                except UnicodeDecodeError:
                    continue
            
            # ë°”ì´ë„ˆë¦¬ íŒŒì¼ì¸ ê²½ìš°
            return None
            
        except Exception:
            return None

    def print_statistics(self):
        """ìŠ¤ìº” ê²°ê³¼ í†µê³„ ì¶œë ¥"""
        total = len(self.projects_data)
        live_sites = sum(1 for p in self.projects_data if p.get("is_live"))
        has_readme = sum(1 for p in self.projects_data if p.get("has_readme"))
        awarded = sum(1 for p in self.projects_data if p.get("award_status") == "awarded")
        
        categories = {}
        quality_scores = [p.get("quality_score", 0) for p in self.projects_data]
        
        for project in self.projects_data:
            cat = project.get("category", "ê¸°íƒ€")
            categories[cat] = categories.get(cat, 0) + 1
        
        print("\n" + "="*50)
        print("ğŸ“Š í”„ë¡œì íŠ¸ ìŠ¤ìº” ê²°ê³¼ í†µê³„")
        print("="*50)
        print(f"ì´ í”„ë¡œì íŠ¸ ìˆ˜: {total}ê°œ")
        print(f"ë¼ì´ë¸Œ ì‚¬ì´íŠ¸: {live_sites}ê°œ ({live_sites/total*100:.1f}%)")
        print(f"README ë³´ìœ : {has_readme}ê°œ ({has_readme/total*100:.1f}%)")
        print(f"ìˆ˜ìƒ í”„ë¡œì íŠ¸: {awarded}ê°œ ({awarded/total*100:.1f}%)")
        print(f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {sum(quality_scores)/len(quality_scores):.1f}ì ")
        
        print("\nğŸ“‚ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬:")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            print(f"  {cat}: {count}ê°œ")

    def save_to_json(self, filename="projects_scan_result.json"):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_path = self.base_path / filename
        
        result = {
            "scan_info": {
                "scan_date": datetime.now().isoformat(),
                "total_projects": len(self.projects_data),
                "scanner_version": "1.0.0"
            },
            "projects": self.projects_data,
            "statistics": self.generate_statistics()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

    def generate_statistics(self):
        """í†µê³„ ë°ì´í„° ìƒì„±"""
        total = len(self.projects_data)
        if total == 0:
            return {}
        
        return {
            "total_projects": total,
            "live_sites": sum(1 for p in self.projects_data if p.get("is_live")),
            "has_readme": sum(1 for p in self.projects_data if p.get("has_readme")),
            "awarded_projects": sum(1 for p in self.projects_data if p.get("award_status") == "awarded"),
            "categories": self.get_category_stats(),
            "quality_distribution": self.get_quality_distribution(),
            "avg_quality_score": sum(p.get("quality_score", 0) for p in self.projects_data) / total
        }

    def get_category_stats(self):
        """ì¹´í…Œê³ ë¦¬ë³„ í†µê³„"""
        categories = {}
        for project in self.projects_data:
            cat = project.get("category", "ê¸°íƒ€")
            categories[cat] = categories.get(cat, 0) + 1
        return categories

    def get_quality_distribution(self):
        """í’ˆì§ˆ ì ìˆ˜ ë¶„í¬"""
        distribution = {"high": 0, "medium": 0, "low": 0}
        
        for project in self.projects_data:
            score = project.get("quality_score", 0)
            if score >= 80:
                distribution["high"] += 1
            elif score >= 50:
                distribution["medium"] += 1
            else:
                distribution["low"] += 1
        
        return distribution


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scanner = ProjectScanner()
    
    print("ğŸš€ MYCLAUDE_PROJECT ìŠ¤ìº” ë„êµ¬ v1.0")
    print("ë°•ìš©í™˜ë‹˜ì˜ ê³µëª¨ì „ í”„ë¡œì íŠ¸ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")
    
    try:
        projects = scanner.scan_all_projects()
        
        if projects:
            print(f"\nâœ… ìŠ¤ìº” ì™„ë£Œ! {len(projects)}ê°œ í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            print("ğŸ“„ ìƒì„¸ ê²°ê³¼ëŠ” 'projects_scan_result.json' íŒŒì¼ì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            print("âŒ ìŠ¤ìº” ì‹¤íŒ¨!")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


if __name__ == "__main__":
    main()
